{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity to Modeling Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "import sys\n",
    "sys.path.insert(0,\"%s/sensitivity_study/src\"%home)\n",
    "from experiment import read_raw_pairwise, construct_support_matrix, get_features_from_support, get_target_stability, eval_models\n",
    "from sensitivity_tests import *\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\",\n",
    "             \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\"]\n",
    "\n",
    "# TODO: measure sensitivity of massey & colley to S construction params\n",
    "\n",
    "config = {\n",
    "    \"col_mapping\": {\n",
    "        \"team1_name\":\"team1_name\",\n",
    "        \"team1_score\":\"points1\",\n",
    "        \"team2_name\":\"team2_name\",\n",
    "        \"team2_score\":\"points2\",\n",
    "        \"team1_select\": \"team1_madness\",\n",
    "        \"team2_select\": \"team2_madness\",\n",
    "        \"date\":\"date\"\n",
    "    },\n",
    "    \"rankingMethods\": [MasseyRankingAlgorithm(), ColleyRankingAlgorithm()],\n",
    "    \"correlationMethod\":utilities.kendall_tau,\n",
    "    \"fracs\": np.linspace(0.5, 1.0, num=21),\n",
    "    \"n_restarts\": 5,\n",
    "    \"direct_thres\": [0, 1, 2, 3, 4, 5],\n",
    "    \"spread_thres\": [0, 1, 2, 3, 4, 5],\n",
    "    \"weight_indirect\": np.linspace(0.1, 1.0, num=10),\n",
    "    \"raw_filepaths\": [\"{}/sensitivity_study/data/MarchMadnessDataFrames/march_madness_{}.csv\".format(home,yr) for yr in years],\n",
    "    \"model_list\": [{\"model\":DummyRegressor(), \"param_grid\": {}},\n",
    "                   {\"model\":LinearRegression(), \"param_grid\": {'fit_intercept': [True, False]}}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = {fp: read_raw_pairwise(fp, config[\"col_mapping\"]) for fp in tqdm(config[\"raw_filepaths\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {\"fp\":[], \"frac\":[], \"method\":[], \"w\":[]}\n",
    "support_matricies = {}\n",
    "rankings_by_method = {}\n",
    "feature_df_list = []\n",
    "# For each raw file (equivalent to a season / tournament / single scenario)\n",
    "# get feature vector and target scalar\n",
    "num_matrices = len(games.keys()) * len(config[\"fracs\"]) * len(config[\"direct_thres\"]) \\\n",
    "               * len(config[\"spread_thres\"]) * len(config[\"weight_indirect\"])\n",
    "with tqdm(total=num_matrices) as pbar:\n",
    "    for fp in tqdm(games.keys()):\n",
    "        for frac in config[\"fracs\"]:\n",
    "            support_matricies[(fp, frac)] = []\n",
    "            rankings_by_method[(fp, frac)] = {r.__class__.__name__: [] for r in config[\"rankingMethods\"]}\n",
    "\n",
    "            for d_thresh, s_thresh, w_ind in itertools.product(config[\"direct_thres\"],\n",
    "                                                               config[\"spread_thres\"],\n",
    "                                                               config[\"weight_indirect\"]):\n",
    "                support_mat = construct_support_matrix(games[fp],\n",
    "                                                       frac,\n",
    "                                                       direct_thres=d_thresh,\n",
    "                                                       spread_thres=s_thresh,\n",
    "                                                       weight_indirect=w_ind)\n",
    "                support_matricies[(fp, frac)].append(support_mat)\n",
    "                # get rankings for support for all ranking methods\n",
    "                for rankingMethod in config[\"rankingMethods\"]:\n",
    "                    rankings_by_method[(fp, frac)][rankingMethod.__class__.__name__].append(rankingMethod.rank(support_mat.fillna(0).values))\n",
    "                pbar.update(1)\n",
    "\n",
    "            for methodName, rankings in rankings_by_method[(fp, frac)].items():\n",
    "                df[\"fp\"].append(fp[-8:-4])\n",
    "                df[\"frac\"].append(frac)\n",
    "                df[\"method\"].append(methodName)\n",
    "                df[\"w\"].append(kendall_w(rankings)[1])\n",
    "\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fp\"] = df[\"fp\"].str[-8:-4] # Turn filename into year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"fp\", \"method\"])[\"w\"].mean().unstack().plot.bar(figsize=(12,5.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in df[\"fp\"].unique():\n",
    "    for method in df[\"method\"].unique():\n",
    "        data = df.loc[(df.fp==yr)&(df.method==method), [\"frac\", \"w\"]]\n",
    "        plt.plot(data.frac, data.w, label=method)\n",
    "    plt.title(\"Sensitivity to Modeling Params ({})\".format(yr))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sensitivity_to_modeling_parameters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = eval_models(features, targets, config[\"model_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = results_dict.keys()\n",
    "maes = [results_dict[model][\"MAE\"] for model in x]\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "plt.bar(x_pos, maes)\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Mean Absolute Error of Regression Models\")\n",
    "\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
