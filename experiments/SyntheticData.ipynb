{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rankability Predicting Sensitivity\n",
    "### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import skew\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"%s/rankability_toolbox_dev\"%home)\n",
    "import pyrankability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,\"%s/sensitivity_study/src\"%home)\n",
    "from sensitivity_tests import *\n",
    "from utilities import *\n",
    "from base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "ELO_scale = 0.25\n",
    "\n",
    "xs = np.linspace(0.0, 1.0, endpoint=True, num=100)\n",
    "ys = sigmoid(xs/ELO_scale)*100.0\n",
    "\n",
    "plt.grid(True, axis=\"both\", which=\"both\", linestyle=\":\")\n",
    "plt.plot(xs, ys)\n",
    "plt.title(\"Skill Difference to P(win) curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synth_games(n_games=748, n_teams=32, n_pts=64):\n",
    "    # Columns are team1, team2, points1, points2, date\n",
    "    df = {\n",
    "        \"team1\":[],\n",
    "        \"team2\":[],\n",
    "        \"team1_name\":[],\n",
    "        \"team2_name\":[],\n",
    "        \"points1\":[],\n",
    "        \"points2\":[]\n",
    "    }\n",
    "    all_teams = range(n_teams)\n",
    "    all_skills = np.linspace(0.0, 1.0, endpoint=True, num=n_teams)\n",
    "    for date in range(n_games):\n",
    "        t1 = date % n_teams\n",
    "        t2 = t1\n",
    "        while t2 == t1:\n",
    "            t2 = random.choice(all_teams)\n",
    "        scaled_elo_diff = (all_skills[t1] - all_skills[t2])/ELO_scale\n",
    "        prob_t1_win = sigmoid(scaled_elo_diff)\n",
    "        t1_pts = n_pts*np.random.binomial(n=1, p=prob_t1_win)\n",
    "        t2_pts = n_pts - t1_pts\n",
    "        df[\"team1\"].append(t1)\n",
    "        df[\"team2\"].append(t2)\n",
    "        df[\"team1_name\"].append(str(t1))\n",
    "        df[\"team2_name\"].append(str(t2))\n",
    "        df[\"points1\"].append(t1_pts)\n",
    "        df[\"points2\"].append(t2_pts)\n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games={}\n",
    "years = list(range(1000))\n",
    "for year in years:\n",
    "    games[year] = generate_synth_games()\n",
    "\n",
    "games[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to future self: Parameters from FODS paper but might need to be optimized\n",
    "direct_thres = 2\n",
    "spread_thres = 2\n",
    "weight_indirect = 0.5\n",
    "Ds = {}\n",
    "#fracs = [0.5, 0.75, 1.] # 25% of total data added per step\n",
    "fracs = [0.512, 0.64, 0.8, 1.] # 25% of current data added per step\n",
    "#fracs = [0.8, 1.0] # simpler case\n",
    "pairs = list(zip(fracs[:-1], fracs[1:]))\n",
    "pair_to_predict = pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in tqdm(games.keys()):\n",
    "    Ds[year] = {}\n",
    "    game_list = list(games[year].index)\n",
    "    \n",
    "    game_df = pd.DataFrame({\"team1_name\":games[year]['team1_name'],\n",
    "                            \"team1_score\":games[year]['points1'],\n",
    "                            \"team2_name\":games[year]['team2_name'],\n",
    "                            \"team2_score\":games[year]['points2']\n",
    "                           })\n",
    "    for frac in fracs:\n",
    "        upper = int(len(game_df)*frac)\n",
    "        game_df_sample = game_df.iloc[:upper,:]\n",
    "        map_func = lambda linked: support_map_vectorized_direct_indirect_weighted(linked,direct_thres=direct_thres,spread_thres=spread_thres,weight_indirect=weight_indirect)\n",
    "        Ds[year][frac] = pyrankability.construct.V_count_vectorized(game_df_sample,map_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ds[0][fracs[0]])\n",
    "\n",
    "plt.imshow(Ds[0][fracs[0]].fillna(0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = {}\n",
    "taus = {}\n",
    "results = pd.DataFrame(columns=pairs+[\"Year\"]).set_index(\"Year\")\n",
    "\n",
    "for year in tqdm(games.keys()):\n",
    "    rankings[year] = []\n",
    "    taus[year] = {}\n",
    "    data = []\n",
    "    for i in range(len(pairs)):\n",
    "        pair = pairs[i]\n",
    "        D1 = Ds[year][pair[0]]\n",
    "        D2 = Ds[year][pair[1]]\n",
    "        ranking1 = MasseyRankingAlgorithm().rank(D1.fillna(0).values)\n",
    "        ranking2 = MasseyRankingAlgorithm().rank(D2.fillna(0).values)\n",
    "        rankings[year].append((ranking1,ranking2))\n",
    "        ranking1, ranking2 = rankings[year][i]\n",
    "        taus[year][pair] = kendall_tau(ranking1,ranking2)\n",
    "        data.append(taus[year][pair])\n",
    "    results = results.append(pd.Series(data,index=results.columns,name=year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pairs = len(pairs)\n",
    "corrs = []\n",
    "for i in range(n_pairs-1):\n",
    "    results.plot.scatter(pairs[i], pairs[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_details = []\n",
    "def get_rankability_results(n_restarts=250):\n",
    "    df_ks = []\n",
    "    df_years = []\n",
    "    df_fracs = []\n",
    "    df_p_stats = {}\n",
    "    for year in tqdm(games.keys()):\n",
    "        for pair in pairs:\n",
    "            D = Ds[year][pair[0]].fillna(0)\n",
    "            #C = pyrankability.construct.C_count(D,0)\n",
    "            k,details = pyrankability.rank.solve(D,method='lop', num_random_restarts=n_restarts, lazy=False, cont=True)\n",
    "            p_stats = get_P_stats(details[\"P\"])\n",
    "            for name, val in p_stats.items():\n",
    "                if name not in df_p_stats:\n",
    "                    df_p_stats[name] = []\n",
    "                df_p_stats[name].append(val)\n",
    "            df_ks.append(k)\n",
    "            df_years.append(year)\n",
    "            df_fracs.append(pair[0])\n",
    "            df_details.append(details)\n",
    "\n",
    "    results_temp = {\"k\":df_ks,\"Year\":df_years,\"Fraction\":df_fracs}\n",
    "    for key, val in df_p_stats.items():\n",
    "        if key in results_temp:\n",
    "            raise ValueError(\"Duplicate column name! Fix collision before moving on!\")\n",
    "        results_temp[key] = val\n",
    "\n",
    "    return pd.DataFrame(results_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankability_results = get_rankability_results()\n",
    "rankability_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_corr = rankability_results.loc[rankability_results.Fraction==pair_to_predict[0]].set_index('Year').join(results)\n",
    "for_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_corr.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(for_corr['k'],for_corr[pair_to_predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\n",
    "    'Year',\n",
    "    '# X* frac',\n",
    "    'k',\n",
    "    '# X* frac top 40',\n",
    "    'kendall_w',\n",
    "    'p_lowerbound',\n",
    "    'max_L2_dist',\n",
    "    'mean_L2_dist',\n",
    "    'min_tau',\n",
    "    'mean_tau',\n",
    "    'Pair'\n",
    "]\n",
    "\n",
    "all_score_df = pd.DataFrame(columns=col_names)\n",
    "\n",
    "c=0\n",
    "for year in tqdm(games.keys()):\n",
    "    for pair in pairs:\n",
    "        V = Ds[year][pair_to_predict[0]]\n",
    "        rresults = rankability_results.iloc[c,:]\n",
    "        k = rresults['k']\n",
    "        details = df_details[c]\n",
    "        x = pd.DataFrame(details['x'],index=V.index,columns=V.columns)\n",
    "        r = x.sum(axis=0)\n",
    "        order = np.argsort(r)\n",
    "        xstar = x.iloc[order,:].iloc[:,order]\n",
    "        xstar.loc[:,:] = pyrankability.common.threshold_x(xstar.values)\n",
    "        inxs = np.triu_indices(len(xstar),k=1)\n",
    "        xstar_upper = xstar.values[inxs[0],inxs[1]]\n",
    "        nfrac_upper = sum((xstar_upper > 0) & (xstar_upper < 1))\n",
    "        flat_frac = ((xstar > 0) & (xstar < 1)).sum(axis=0)\n",
    "        nfrac_top_40 = flat_frac.iloc[:40].sum()\n",
    "        entry_data = [\n",
    "            year,\n",
    "            nfrac_upper*2,\n",
    "            k,\n",
    "            nfrac_top_40,\n",
    "            rresults[\"kendall_w\"],\n",
    "            rresults[\"p_lowerbound\"],\n",
    "            rresults[\"max_L2_dist\"],\n",
    "            rresults[\"mean_L2_dist\"],\n",
    "            rresults[\"min_tau\"],\n",
    "            rresults[\"mean_tau\"],\n",
    "            pair\n",
    "        ]\n",
    "        entry = pd.Series(entry_data,col_names,name=c)\n",
    "        c+=1\n",
    "        all_score_df = all_score_df.append(entry)\n",
    "all_score_df.set_index(\"Year\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score_df = all_score_df.loc[all_score_df.Pair == pair_to_predict].drop('Pair',axis=1).join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score_df.to_csv(\"all_synth_score_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score_df = pd.read_csv(\"all_synth_score_df.csv\")\n",
    "all_score_df = all_score_df.set_index(\"Year\")\n",
    "\n",
    "# All the pairs were read in as strings\n",
    "pair_to_predict = str(pair_to_predict)\n",
    "pairs = [str(p) for p in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in all_score_df.columns:\n",
    "    if col not in pairs:\n",
    "        all_score_df.plot.scatter(col, pair_to_predict, title=\"Final Sensitivity vs \" + col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "all_feature_cols = [c for c in all_score_df.columns if c not in pairs]\n",
    "\n",
    "def evaluate(df,pred_col=pair_to_predict,feature_cols=all_feature_cols,model=DummyRegressor(),param_grid={}):\n",
    "    # fill in evaluat\n",
    "    loo = LeaveOneOut()\n",
    "    y = df[pred_col]\n",
    "    X = df[feature_cols]\n",
    "    \n",
    "    grid = GridSearchCV(model,param_grid,refit=True,verbose=0, cv=3, iid=True, n_jobs=-1)\n",
    "    scores = cross_val_score(grid, X, y, scoring=\"neg_mean_absolute_error\", cv=loo, n_jobs=4, verbose=0)\n",
    "    return pd.Series([len(scores),np.mean(np.abs(scores)),np.std(scores)],index=[\"Folds\",\"MAE\",\"STD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Evaluates a regression model attempting to regress \"pred_col\" using leave-one-out\n",
    "#     cross validation. If the model has parameters values to search over, each unique\n",
    "#     parameter setting will be evaluated using 3-fold cross validation on top of the LOO CV.\n",
    "# Reported statistics are [mean of absolute error] and [std of absolute error] over all LOO folds.\n",
    "def evaluate_exhaustive(df,\n",
    "                        pred_col=pair_to_predict,\n",
    "                        feature_cols=all_feature_cols,\n",
    "                        model=DummyRegressor(),\n",
    "                        param_grid={},\n",
    "                        print_best_params=False):\n",
    "    exhaustive = {}\n",
    "    y = df[pred_col]\n",
    "    X = df[feature_cols]\n",
    "    \n",
    "    # run on subsets of features\n",
    "    exhaustive_feat_select = list(chain.from_iterable(combinations(list(range(len(X.columns))), r) for r in range(len(X.columns))))[1:]\n",
    "    # only 10 feature subsets (out of 2^n) for debug purposes\n",
    "    best_score = np.Inf\n",
    "    best_features = None\n",
    "    for ps in tqdm(exhaustive_feat_select):\n",
    "        features = X.iloc[:, list(ps)]\n",
    "        grid = GridSearchCV(model,param_grid,refit=True,verbose=0, cv=3, iid=True, n_jobs=-1)\n",
    "        exhaustive[ps] = np.mean(np.abs(cross_val_score(grid, features, y, scoring=\"neg_mean_absolute_error\", cv=LeaveOneOut(), n_jobs=1)))\n",
    "        if exhaustive[ps] < best_score:\n",
    "            best_score = exhaustive[ps]\n",
    "            best_features = ps\n",
    "    \n",
    "    # print(scores)\n",
    "    return {\"MAE\": best_score, \"best_feature_subset\": [X.columns[f] for f in best_features]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model_results = evaluate(all_score_df)\n",
    "dummy_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model_results = evaluate(all_score_df,model=SVR(gamma='scale'),param_grid = {'C': [0.1,1,10,100], 'epsilon': [0.1,0.5,1],'kernel': ['linear', 'rbf']})\n",
    "svr_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_results = evaluate_exhaustive(all_score_df,model=LinearRegression(),param_grid = {'fit_intercept': [True, False]})\n",
    "lr_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['Baseline', 'SVR', \"Linear Regression\"]\n",
    "maes = [dummy_model_results[\"MAE\"], svr_model_results[\"MAE\"], lr_model_results[\"MAE\"]]\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "plt.bar(x_pos, maes)\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Mean Absolute Error of Regression Models\")\n",
    "\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
