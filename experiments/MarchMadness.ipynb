{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rankability Predicting Sensitivity\n",
    "## March Madness Dataset\n",
    "\n",
    "Look for new features:</br>\n",
    "    \n",
    "Existing features:</br>\n",
    "    Diversity of P metrics</br>\n",
    "    Graph measures of tournament matrix as Lapacian</br>\n",
    "    Eigenvalues of tournament matrix</br>\n",
    "    \n",
    "Datasets:</br>\n",
    "   Lichess:</br>\n",
    "       API: https://berserk.readthedocs.io/en/master/ </br>\n",
    "       Look for tournaments, grab games played in that time frame and create D matricies</br>\n",
    "       API is pretty simple we just need to build a scraping script.</br>\n",
    "   \n",
    "   Sumo:\n",
    "   Data: https://data.world/cervus/sumo-results </br>\n",
    "   It's literally just CSVs, so grab to PANDAS and build D from columns\n",
    "   Bad news: Have to make an account to download data :( /s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of the Experiment\n",
    "\n",
    " 1. Load in the game-by-game data for each year from 2002-2018\n",
    " 2. For each year, construct multiple D matrices using increasing percentages of the games for that year.</br>_(For instance in the simplest case, construct a D matrix using only the first half of the season, then a D matrix with the full season.)_\n",
    " 2. Produce Massey rankings for each D matrix and calculate the Kendall tau between rankings from the same year </br>_(These kendall taus represent the amount that the ranking changed when more data was included)_\n",
    " 3. For each year, measure features of the restricted dataset (in the simple case, D constructed from 50% of the games) and create a dataset of these early-measurable features.\n",
    " 4. Evaluate whether these early-measurable features can be used to predict the amount that rankings changed after including more data (Kendall taus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import skew\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"%s/rankability_toolbox_dev\"%home)\n",
    "import pyrankability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,\"%s/sensitivity_study/src\"%home)\n",
    "from sensitivity_tests import *\n",
    "from utilities import *\n",
    "from base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to future self: Parameters from FODS paper but might need to be optimized\n",
    "direct_thres = 2\n",
    "spread_thres = 2\n",
    "weight_indirect = 0.5\n",
    "Ds = {}\n",
    "# fracs represent how much of the data to include\n",
    "fracs = [0.75, 1.] # 25% of total data added\n",
    "pairs = list(zip(fracs[:-1], fracs[1:]))\n",
    "pair_to_predict = pairs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games={}\n",
    "years = [\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\"]\n",
    "for year in years:\n",
    "    games[year] = pd.read_csv(\"../data/MarchMadnessDataFrames/march_madness_%s.csv\"%year)\n",
    "print(year)\n",
    "games[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in tqdm(games.keys()):\n",
    "    Ds[year] = {}\n",
    "    madness_teams = np.unique(list(games[year].team1_name.loc[games[year].team1_madness == 1]) + list(games[year].team2_name.loc[games[year].team2_madness == 1]))\n",
    "    game_list = list(games[year].index)\n",
    "    \n",
    "    game_df = pd.DataFrame({\"team1_name\":games[year]['team1_name'],\n",
    "                            \"team1_score\":games[year]['points1'],\n",
    "                            \"team1_H_A_N\": games[year]['H_A_N1'],\n",
    "                            \"team2_name\":games[year]['team2_name'],\n",
    "                            \"team2_score\":games[year]['points2'],\n",
    "                            \"team2_H_A_N\": games[year]['H_A_N1'],\n",
    "                            \"date\": games[year]['date']\n",
    "                           }).sort_values(by='date').drop('date',axis=1)\n",
    "    for frac in fracs:\n",
    "        upper = int(len(game_df)*frac)\n",
    "        game_df_sample = game_df.iloc[:upper,:]\n",
    "        # support_map_vectorized_direct_indirect_weighted implements our common approach to looking for evidence of direct and indirect dominance\n",
    "        # I'm just using an annonymous function because the helper function V_count_vectorized expects a function with one argument\n",
    "        map_func = lambda linked: pyrankability.construct.support_map_vectorized_direct_indirect_weighted(linked,direct_thres=direct_thres,spread_thres=spread_thres,weight_indirect=weight_indirect)\n",
    "        Ds[year][frac] = pyrankability.construct.V_count_vectorized(game_df_sample,map_func).loc[madness_teams,madness_teams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = {}\n",
    "taus = {}\n",
    "results = pd.DataFrame(columns=pairs+[\"Year\"]).set_index(\"Year\")\n",
    "\n",
    "for year in tqdm(games.keys()):\n",
    "    rankings[year] = []\n",
    "    taus[year] = {}\n",
    "    data = []\n",
    "    for i in range(len(pairs)):\n",
    "        pair = pairs[i]\n",
    "        D1 = Ds[year][pair[0]]\n",
    "        D2 = Ds[year][pair[1]]\n",
    "        ranking1 = MasseyRankingAlgorithm().rank(D1.fillna(0).values)\n",
    "        ranking2 = MasseyRankingAlgorithm().rank(D2.fillna(0).values)\n",
    "        rankings[year].append((ranking1,ranking2))\n",
    "        ranking1, ranking2 = rankings[year][i]\n",
    "        taus[year][pair] = kendall_tau(ranking1,ranking2)\n",
    "        data.append(taus[year][pair])\n",
    "    results = results.append(pd.Series(data,index=results.columns,name=year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: what do the contents of this matrix mean??\n",
    "# Ds['2018'][1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_details = []\n",
    "# This function constructs a dataframe of features\n",
    "# (collected from the first D matrix of each pair)\n",
    "# To be used to predict the movement of the pair (kendall tau of rankings)\n",
    "def get_rankability_results(n_restarts=250):\n",
    "    df_ks = []\n",
    "    df_years = []\n",
    "    df_fracs = []\n",
    "    df_p_stats = {}\n",
    "    for year in tqdm(games.keys()):\n",
    "        D = Ds[year][pair_to_predict[0]].fillna(0)\n",
    "        k,details = pyrankability.rank.solve(D,method='lop', num_random_restarts=n_restarts, lazy=False, cont=True)\n",
    "        p_stats = get_P_stats(details[\"P\"])\n",
    "        for name, val in p_stats.items():\n",
    "            if name not in df_p_stats:\n",
    "                df_p_stats[name] = []\n",
    "            df_p_stats[name].append(val)\n",
    "        df_ks.append(k)\n",
    "        df_years.append(year)\n",
    "        df_fracs.append(pair_to_predict[0])\n",
    "        df_details.append(details)\n",
    "\n",
    "    results_temp = {\"k\":df_ks,\"Year\":df_years,\"Fraction\":df_fracs}\n",
    "    for key, val in df_p_stats.items():\n",
    "        if key in results_temp:\n",
    "            raise ValueError(\"Duplicate column name! Fix collision before moving on!\")\n",
    "        results_temp[key] = val\n",
    "\n",
    "    return pd.DataFrame(results_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankability_results = get_rankability_results(n_restarts=2)\n",
    "rankability_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_corr = rankability_results.loc[rankability_results.Fraction==pair_to_predict[0]].set_index('Year').join(results)\n",
    "for_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_corr.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(for_corr['k'],for_corr[pair_to_predict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes Ethan 9/27/20\n",
    "\n",
    "Determinant and trace of Ds not good features <br/>\n",
    "Max and min eigenvalue not bad <br/>\n",
    "\n",
    "Attempted betweenness centrality features, none worthwhile <br/>\n",
    "\n",
    "`betweennesses = nx.betweenness_centrality(dsGraph)\n",
    "avg_bt_centrality = np.average(np.array(list(betweennesses.values())))\n",
    "var_bt_centrality = np.sqrt(np.var(np.array(list(betweennesses.values()))))\n",
    "print(avg_bt_centrality, var_bt_centrality)`\n",
    "\n",
    "Notes Ethan 10/1/20:\n",
    "Feats: x_star eigenvals, min and max, matrix norm of x_star, etc.\n",
    "Feature idea: Cut data in half, run alg at 25% mark and see what we would predict for .25,.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\n",
    "    'Year',\n",
    "    '# X* frac',\n",
    "    'k',\n",
    "    '# X* frac top 40',\n",
    "    'kendall_w',\n",
    "    'p_lowerbound',\n",
    "    'max_L2_dist',\n",
    "    'mean_L2_dist',\n",
    "    'min_tau',\n",
    "    'mean_tau',\n",
    "    'max_eigenval',\n",
    "    'min_eigenval',\n",
    "    'max_eigenval_xstar',\n",
    "    'min_eigenval_xstar',\n",
    "    'Pair'\n",
    "]\n",
    "\n",
    "all_score_df = pd.DataFrame(columns=col_names)\n",
    "\n",
    "c=0\n",
    "for year in tqdm(games.keys()):\n",
    "    # the support matrix for a pair for a given year\n",
    "    pair_to_predict = (.75, 1.0)\n",
    "    V = Ds[year][pair_to_predict[0]]\n",
    "    # print(V.fillna(0.0))\n",
    "    vals, vecs = np.linalg.eig(V.fillna(0.0).to_numpy())\n",
    "    determinant = np.prod(vals)\n",
    "    trace = np.sum(vals)\n",
    "    max_eigenval = np.real(np.max(vals))\n",
    "    min_eigenval = np.real(np.min(vals))\n",
    "    dsGraph = nx.from_numpy_matrix(V.fillna(0.0).to_numpy())\n",
    "    \n",
    "    rresults = rankability_results.iloc[c,:]\n",
    "    k = rresults['k']\n",
    "    details = df_details[c]\n",
    "    x = pd.DataFrame(details['x'],index=V.index,columns=V.columns)\n",
    "    r = x.sum(axis=0)\n",
    "    order = np.argsort(r)\n",
    "    xstar = x.iloc[order,:].iloc[:,order]\n",
    "    xstar.loc[:,:] = pyrankability.common.threshold_x(xstar.values)\n",
    "    # print(xstar.values)\n",
    "    print(np.linalg.norm(xstar.values, \"fro\"), 'fro')\n",
    "    vals, vecs = np.linalg.eig(xstar.to_numpy())\n",
    "    det_xstar = np.real(np.prod(vals))\n",
    "    print(\"det\", det_xstar)\n",
    "    max_eigenval_xstar = np.real(np.max(vals))\n",
    "    min_eigenval_xstar = np.real(np.min(vals))\n",
    "    print(max_eigenval_xstar)\n",
    "    print(min_eigenval_xstar)\n",
    "    \n",
    "    inxs = np.triu_indices(len(xstar),k=1)\n",
    "    xstar_upper = xstar.values[inxs[0],inxs[1]]\n",
    "    nfrac_upper = sum((xstar_upper > 0) & (xstar_upper < 1))\n",
    "    flat_frac = ((xstar > 0) & (xstar < 1)).sum(axis=0)\n",
    "    nfrac_top_40 = flat_frac.iloc[:40].sum()\n",
    "    entry_data = [\n",
    "        year,\n",
    "        nfrac_upper*2,\n",
    "        k,\n",
    "        nfrac_top_40,\n",
    "        rresults[\"kendall_w\"],\n",
    "        rresults[\"p_lowerbound\"],\n",
    "        rresults[\"max_L2_dist\"],\n",
    "        rresults[\"mean_L2_dist\"],\n",
    "        rresults[\"min_tau\"],\n",
    "        rresults[\"mean_tau\"],\n",
    "        max_eigenval, \n",
    "        min_eigenval,\n",
    "        max_eigenval_xstar,\n",
    "        min_eigenval_xstar,\n",
    "        pair\n",
    "    ]\n",
    "    entry = pd.Series(entry_data,col_names,name=c)\n",
    "    c+=1\n",
    "    all_score_df = all_score_df.append(entry)\n",
    "all_score_df.set_index(\"Year\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score_df = all_score_df.loc[all_score_df.Pair == pair_to_predict].drop('Pair',axis=1).join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By this point, all_score_df includes all features that will be used to predict the sensitivity to new information\n",
    "all_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score_df.to_csv(\"all_score_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only run if you need a_s_d from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score_df = pd.read_csv(\"all_score_df.csv\")\n",
    "all_score_df = all_score_df.set_index(\"Year\")\n",
    "\n",
    "# All the pairs have become strings after being read in\n",
    "pair_to_predict = str(pair_to_predict)\n",
    "pairs = [str(p) for p in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score_df.columns\n",
    "cdf = all_score_df.fillna(0.0).corr()\n",
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = all_score_df.fillna(0.0).corr()\n",
    "print(cdf.columns)\n",
    "for col in all_score_df.columns:\n",
    "    if col not in pairs:\n",
    "        # all_score_df.plot.scatter(col, pair_to_predict, title=\"Final Sensitivity vs \" + col)\n",
    "        print(\"Correlation between\", pair_to_predict, \"and\", col, cdf[pair_to_predict].loc[col], \"R^2\", cdf[pair_to_predict].loc[col]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# List of all features to use to predict\n",
    "# (drops the quantity to be predicted)\n",
    "all_feature_cols = [c for c in all_score_df.columns if c not in pairs]\n",
    "\n",
    "# Evaluates a regression model attempting to regress \"pred_col\" using leave-one-out\n",
    "#     cross validation. If the model has parameters values to search over, each unique\n",
    "#     parameter setting will be evaluated using 3-fold cross validation on top of the LOO CV.\n",
    "# Reported statistics are [mean of absolute error] and [std of absolute error] over all LOO folds.\n",
    "def evaluate(df,pred_col=pair_to_predict,feature_cols=all_feature_cols,model=DummyRegressor(),param_grid={}):\n",
    "    loo = LeaveOneOut()\n",
    "    pred_col = str(pred_col)\n",
    "    y = df[pred_col]\n",
    "    X = df[feature_cols]\n",
    "    \n",
    "    grid = GridSearchCV(model,param_grid,refit=True,verbose=0, cv=3, iid=True, n_jobs=-1)\n",
    "    scores = cross_val_score(grid, X, y, scoring=\"neg_mean_absolute_error\", cv=loo, n_jobs=1)\n",
    "    return pd.Series([len(scores),np.mean(np.abs(scores)),np.std(scores)],index=[\"Folds\",\"MAE\",\"STD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.append((.5, .75))\n",
    "pairs.append((.75, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above but with exhaustive feature selection\n",
    "\n",
    "Below:\n",
    "Seems to be a bug with feature subset selection. All scores come out the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_cols = [c for c in all_score_df.columns if c not in pairs]\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Evaluates a regression model attempting to regress \"pred_col\" using leave-one-out\n",
    "#     cross validation. If the model has parameters values to search over, each unique\n",
    "#     parameter setting will be evaluated using 3-fold cross validation on top of the LOO CV.\n",
    "# Reported statistics are [mean of absolute error] and [std of absolute error] over all LOO folds.\n",
    "def evaluate_exhaustive(df,\n",
    "                        pred_col=pair_to_predict,\n",
    "                        feature_cols=all_feature_cols,\n",
    "                        model=DummyRegressor(),\n",
    "                        param_grid={},\n",
    "                        print_best_params=False):\n",
    "    exhaustive = {}\n",
    "    y = df[pred_col]\n",
    "    X = df[feature_cols]\n",
    "    \n",
    "    # run on subsets of features\n",
    "    exhaustive_feat_select = list(chain.from_iterable(combinations(list(range(len(X.columns))), r) for r in range(len(X.columns))))[1:3000]\n",
    "    # only 10 feature subsets (out of 2^n) for debug purposes\n",
    "    best_score = np.Inf\n",
    "    best_features = None\n",
    "    for ps in tqdm(exhaustive_feat_select, ascii=True):\n",
    "        features = X.iloc[:, list(ps)]\n",
    "        grid = GridSearchCV(model,param_grid,refit=True,verbose=0, cv=3, iid=True, n_jobs=-1)\n",
    "        exhaustive[ps] = np.mean(np.abs(cross_val_score(grid, features, y, scoring=\"neg_mean_absolute_error\", cv=3, n_jobs=1)))\n",
    "        if exhaustive[ps] < best_score:\n",
    "            best_score = exhaustive[ps]\n",
    "            best_features = ps\n",
    "    \n",
    "    # print(scores)\n",
    "    return ({\"MAE\": best_score, \"best_feature_subset\": [X.columns[f] for f in best_features]}, exhaustive)\n",
    "all_feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model_results = evaluate(all_score_df,model=SVR(),param_grid = {'C': [1,10,100], 'epsilon': [.01, 0.1],'kernel': ['linear', 'rbf']})\n",
    "svr_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_score_df[all_feature_cols]\n",
    "y = all_score_df[(.5, .75)]\n",
    "grid = GridSearchCV(model,param_grid,refit=True,verbose=0, cv=3, iid=True, n_jobs=-1)\n",
    "np.mean(np.abs(cross_val_score(grid, X.iloc[:, [5, 6, 7]], y, scoring=\"neg_mean_absolute_error\", cv=3, n_jobs=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_results, lr_model_scores = evaluate_exhaustive(all_score_df,pred_col=(.5, .75),model=LinearRegression(),param_grid = {'fit_intercept': [True, False]})\n",
    "lr_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model_results, dummyscores = evaluate_exhaustive(all_score_df, pred_col=(.5, .75))\n",
    "dummy_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_model_result_score = {'MAE': abs(np.max(np.array([max(i) for i in dummy_model_results.values])))}\n",
    "dummy_model_result_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['Baseline', \"Linear Regression\"]\n",
    "maes = [dummy_model_results[\"MAE\"], lr_model_results[\"MAE\"]]\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "plt.bar(x_pos, maes)\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Mean Absolute Error of Regression Models\")\n",
    "\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
