{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rankability Predicting Sensitivity\n",
    "## March Madness Dataset\n",
    "\n",
    "Goal of this notebook is to analyze and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import skew\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter-pander14'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"%s/rankability_toolbox_dev\"%home)\n",
    "import pyrankability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,\"%s/sensitivity_study/src\"%home)\n",
    "from sensitivity_tests import *\n",
    "from utilities import *\n",
    "from base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_1 = joblib.load(\"/disk/rankability_datasets/sensitivity_study/problem_0001.joblib.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This dataset represents the sensitivity problem as defined as follows:\n",
      "\n",
      "A practitioner wants to predict the degree to which a season of the NCAA Menâ€™s Basketball \n",
      "will be sensitive to choice of Massey specific ranking pipeline throughout the entire season beginning at the \n",
      "half way mark. The goal is to do so without resorting to full or partial enumeration of all possible choices. \n",
      "True sensitivity of a season will be measured as the mean top 10 intersection over the cartesian product \n",
      "of algorithms and parameters.\n",
      "\n",
      "direct_thress = [0,1,2]\n",
      "spread_thress = [0,3,6]\n",
      "weight_indirects = [0.25,0.5,1.]\n",
      "domains_ranges = [('all','madness'),('madness','madness')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(problem_1[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year\n",
       "2002    0.835684\n",
       "2003    0.840171\n",
       "2004    0.860470\n",
       "2005    0.849786\n",
       "2006    0.847863\n",
       "2007    0.869872\n",
       "2008    0.855128\n",
       "2009    0.863034\n",
       "2010    0.867949\n",
       "2011    0.842308\n",
       "2012    0.805556\n",
       "2013    0.804701\n",
       "2014    0.857051\n",
       "2015    0.862393\n",
       "2016    0.837607\n",
       "2017    0.840812\n",
       "2018    0.845085\n",
       "Name: mean_top10_intersection, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_1['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_1['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['frac=0.5', 'frac=0.6', 'frac=0.7', 'frac=0.8', 'frac=0.9', 'frac=1.0'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_1['data']['2002'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = problem_1\n",
    "years = list(problem['target'].index)\n",
    "remaining_games = problem['other']['remaining_games']\n",
    "madness_teams = problem['other']['madness_teams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to search\n",
    "direct_thress = [0,1,2]\n",
    "spread_thress = [0,3,6]\n",
    "weight_indirects = [0.25,0.5]\n",
    "domains_ranges = [('all','madness'),('madness','madness')]\n",
    "\n",
    "years_train = ['2002','2003','2004','2005','2006']\n",
    "years_test = copy.copy(years)\n",
    "for year in years_train:\n",
    "    years_test.remove(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(game_df,team_range,direct_thres,spread_thres,weight_indirect):\n",
    "    hillside_columns = [\"direct_thres\",\"spread_thres\",\"weight_indirect\",\"details\"]\n",
    "\n",
    "    map_func = lambda linked: pyrankability.construct.support_map_vectorized_direct_indirect_weighted(linked,direct_thres=direct_thres,spread_thres=spread_thres,weight_indirect=weight_indirect)\n",
    "    D = pyrankability.construct.V_count_vectorized(game_df,map_func).reindex(index=team_range,columns=team_range)\n",
    "    k,details = pyrankability.rank.solve(D,method='hillside',lazy=False,cont=True)\n",
    "    x = pd.DataFrame(details['x'],columns=D.columns,index=D.index)\n",
    "    c = pd.DataFrame(pyrankability.construct.C_count(D),columns=D.columns,index=D.index)\n",
    "    P = details['P']\n",
    "    simple_details = {'k':k,'x':x,'c':c,'P':P,'D':D}\n",
    "    hillside_ret = pd.Series([direct_thres,spread_thres,weight_indirect,simple_details],index=hillside_columns)\n",
    "    return hillside_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def score_by_correlation(s,r):\n",
    "    return pearsonr(s,r)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"Score\",\"Parameters\"])\n",
    "columns = results.columns\n",
    "results.set_index('Parameters',inplace=True)\n",
    "\n",
    "outer_keys = list(itertools.product(direct_thress,spread_thress,weight_indirects,domains_ranges))\n",
    "\n",
    "def calc_score(y,score_by,direct_thres,spread_thres,weight_indirect,domain_range):\n",
    "    parameter_string = f\"{domain_range},dt={direct_thres},st={spread_thres},iw={weight_indirect}\"\n",
    "    values = []\n",
    "    for year in y.index:\n",
    "        # set the team_range\n",
    "        team_range = None\n",
    "        if domain_range[1] == 'madness':\n",
    "            team_range = madness_teams[year]\n",
    "        elif domain_range[1] == 'all':\n",
    "            team_range = all_teams[year]\n",
    "        elif \"top\" in domain_range:\n",
    "            team_range = all_teams[year]\n",
    "        \n",
    "        knorms = []\n",
    "        for frac_key in problem['data'][year].keys():\n",
    "            hillside_details = compute(problem['data'][year][frac_key],team_range,direct_thres,spread_thres,weight_indirect)\n",
    "            n = len(hillside_details['details']['D'])\n",
    "            kmax = (n*n-n)/2 * n\n",
    "            knorms.append(hillside_details['details']['k']/kmax)\n",
    "        knorm = np.mean(knorms)\n",
    "        values.append(knorm)\n",
    "    return pd.DataFrame([[score_by(values,y),parameter_string]],columns=columns)\n",
    "#direct_thres,spread_thres,weight_indirect,domain_range=outer_keys[0]\n",
    "#calc_score(problem['target'],score_by_correlation,direct_thres,spread_thres,weight_indirect,domain_range)\n",
    "job_results = Parallel(n_jobs=-1)(delayed(calc_score)(problem['target'].loc[years_train],score_by_correlation,direct_thres,spread_thres,weight_indirect,domain_range) for direct_thres,spread_thres,weight_indirect,domain_range in outer_keys)\n",
    "for jresults in job_results:\n",
    "    jresults = jresults.set_index('Parameters')\n",
    "    results = results.append(jresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = results.idxmin()\n",
    "results.loc[best_parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_str = list(best_parameters)[0]\n",
    "best_parameters_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_string_to_parameters(parameter_string):\n",
    "    cmd = parameter_string.split(')')[0]+\")\"\n",
    "    fields = cmd.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").split(\",\")\n",
    "    domain_range = tuple(fields)\n",
    "    exec(parameter_string.split(\"),\")[1].replace(\",\",\";\").strip())\n",
    "    return domain_range,dt,st,iw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(columns=[\"Score\",\"Parameters\"])\n",
    "columns = test_results.columns\n",
    "test_results.set_index('Parameters',inplace=True)\n",
    "\n",
    "domain_range,direct_thres,spread_thres,weight_indirect=parameter_string_to_parameters(best_parameters_str)\n",
    "test_results = calc_score(problem['target'].loc[years_test],score_by_correlation,direct_thres,spread_thres,weight_indirect,domain_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
